{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "# numpy\n",
    "# scikit-learn\n",
    "# transformers\n",
    "# torch\n",
    "\n",
    "# when installing torch\n",
    "\n",
    "# Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
    "# Successfully installed MarkupSafe-2.1.5 jinja2-3.1.5 mpmath-1.3.0 networkx-3.1 sympy-1.13.3 torch-2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18536\\1176109446.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.4.1-cp38-cp38-win_amd64.whl.metadata (27 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\game\\anaconda3\\envs\\grover\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\game\\anaconda3\\envs\\grover\\lib\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp38-cp38-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp38-cp38-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp38-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\game\\anaconda3\\envs\\grover\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\game\\anaconda3\\envs\\grover\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp38-cp38-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Using cached torch-2.4.1-cp38-cp38-win_amd64.whl (199.4 MB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached PyYAML-6.0.2-cp38-cp38-win_amd64.whl (162 kB)\n",
      "Using cached regex-2024.11.6-cp38-cp38-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached tokenizers-0.20.3-cp38-none-win_amd64.whl (2.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp38-cp38-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl (17 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: pytz, mpmath, urllib3, tzdata, typing-extensions, tqdm, sympy, safetensors, regex, pyyaml, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, pandas, jinja2, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.12.14 charset-normalizer-3.4.1 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.1 idna-3.10 jinja2-3.1.5 mpmath-1.3.0 networkx-3.1 numpy-1.24.4 pandas-2.0.3 pytz-2024.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 sympy-1.13.3 tokenizers-0.20.3 torch-2.4.1 tqdm-4.67.1 transformers-4.46.3 typing-extensions-4.12.2 tzdata-2025.1 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromosome</th>\n",
       "      <th>start_of_bin</th>\n",
       "      <th>end_of_bin</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>168067890</td>\n",
       "      <td>168068890</td>\n",
       "      <td>0</td>\n",
       "      <td>GTGAGCCTGGGGCTGCTCTGAGCTGAGCCTTGAGCCCAGGCCAGGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr17</td>\n",
       "      <td>45355603</td>\n",
       "      <td>45356603</td>\n",
       "      <td>0</td>\n",
       "      <td>ATATTGACTCTCGCTCTCCAAGAAGAGTATGAAATAATAGTAGTTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr19</td>\n",
       "      <td>11551317</td>\n",
       "      <td>11552317</td>\n",
       "      <td>1</td>\n",
       "      <td>AGTGGCATGATCTTTGCTTACCCCAACCTCCACATCCGGGTTCAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chrX</td>\n",
       "      <td>38034101</td>\n",
       "      <td>38035101</td>\n",
       "      <td>1</td>\n",
       "      <td>TATGCACATTCTCTCTAAAATGTTGTAATACTGGCATTAATCTAAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr4</td>\n",
       "      <td>28445075</td>\n",
       "      <td>28446075</td>\n",
       "      <td>0</td>\n",
       "      <td>TTATCCTAATGCATGAATTTTCATCTAATACATGAAAGGAGAAAAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85482</th>\n",
       "      <td>chrY</td>\n",
       "      <td>27637470</td>\n",
       "      <td>27638470</td>\n",
       "      <td>0</td>\n",
       "      <td>TGGCCTGACAGCCTCTACCAGTCCTGCTGTCCCTTGGCTGAGAAAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85483</th>\n",
       "      <td>chrY</td>\n",
       "      <td>28133554</td>\n",
       "      <td>28134554</td>\n",
       "      <td>0</td>\n",
       "      <td>ACCACAGGCATGCACCACCAAGCTCAGCTTATTTTTTTTGTATTTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85484</th>\n",
       "      <td>chrY</td>\n",
       "      <td>28168346</td>\n",
       "      <td>28169346</td>\n",
       "      <td>0</td>\n",
       "      <td>AGAAGAAGCAGAGTTCTATTTGGCATTAAAACGGTTCAAAGTGTTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85485</th>\n",
       "      <td>chrY</td>\n",
       "      <td>28427226</td>\n",
       "      <td>28428226</td>\n",
       "      <td>0</td>\n",
       "      <td>TAGTGCCTGGCATAAAGTAAGCACGTATTGAGTAAAGTAAACACTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85486</th>\n",
       "      <td>chrY</td>\n",
       "      <td>58985259</td>\n",
       "      <td>58986259</td>\n",
       "      <td>0</td>\n",
       "      <td>TAACTTTTCCAGTCTCTGCCTACAGAGGGCGTTGTGACATCATTCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85487 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chromosome  start_of_bin  end_of_bin  label  \\\n",
       "0           chr1     168067890   168068890      0   \n",
       "1          chr17      45355603    45356603      0   \n",
       "2          chr19      11551317    11552317      1   \n",
       "3           chrX      38034101    38035101      1   \n",
       "4           chr4      28445075    28446075      0   \n",
       "...          ...           ...         ...    ...   \n",
       "85482       chrY      27637470    27638470      0   \n",
       "85483       chrY      28133554    28134554      0   \n",
       "85484       chrY      28168346    28169346      0   \n",
       "85485       chrY      28427226    28428226      0   \n",
       "85486       chrY      58985259    58986259      0   \n",
       "\n",
       "                                                sequence  \n",
       "0      GTGAGCCTGGGGCTGCTCTGAGCTGAGCCTTGAGCCCAGGCCAGGA...  \n",
       "1      ATATTGACTCTCGCTCTCCAAGAAGAGTATGAAATAATAGTAGTTA...  \n",
       "2      AGTGGCATGATCTTTGCTTACCCCAACCTCCACATCCGGGTTCAAG...  \n",
       "3      TATGCACATTCTCTCTAAAATGTTGTAATACTGGCATTAATCTAAA...  \n",
       "4      TTATCCTAATGCATGAATTTTCATCTAATACATGAAAGGAGAAAAA...  \n",
       "...                                                  ...  \n",
       "85482  TGGCCTGACAGCCTCTACCAGTCCTGCTGTCCCTTGGCTGAGAAAC...  \n",
       "85483  ACCACAGGCATGCACCACCAAGCTCAGCTTATTTTTTTTGTATTTT...  \n",
       "85484  AGAAGAAGCAGAGTTCTATTTGGCATTAAAACGGTTCAAAGTGTTG...  \n",
       "85485  TAGTGCCTGGCATAAAGTAAGCACGTATTGAGTAAAGTAAACACTC...  \n",
       "85486  TAACTTTTCCAGTCTCTGCCTACAGAGGGCGTTGTGACATCATTCT...  \n",
       "\n",
       "[85487 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTCF_dataset = pd.read_csv('CTCF_dataset.tsv', sep='\\t')\n",
    "CTCF_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"PoetschLab/GROVER\")\n",
    "model = AutoModel.from_pretrained(\"PoetschLab/GROVER\")\n",
    "print(\"Model successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "inputs = tokenizer(dna, return_tensors = 'pt')[\"input_ids\"]\n",
    "hidden_states = model(inputs)[0]\n",
    "print(\"Hidden states successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings successfully loaded.\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "embedding_mean = torch.mean(hidden_states[0], dim=0)\n",
    "print(\"Embeddings successfully loaded.\")\n",
    "print(embedding_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedDataset(Dataset):\n",
    "#     \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "#     def __init__(self, texts, labels, tokenizer):\n",
    "\n",
    "#         super(SupervisedDataset, self).__init__()\n",
    "\n",
    "#         sequences = [text for text in texts]\n",
    "\n",
    "#         output = tokenizer(\n",
    "#             sequences,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=310,\n",
    "#             padding=\"longest\",\n",
    "#             return_tensors=\"pt\",\n",
    "#             truncation=True\n",
    "#         )\n",
    "\n",
    "#         self.input_ids = output[\"input_ids\"]\n",
    "#         self.attention_mask = output[\"attention_mask\"]\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         return dict(\n",
    "#             input_ids=self.input_ids[i],\n",
    "#             labels=self.labels[i],\n",
    "#             attention_mask=self.attention_mask[i]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SupervisedDataset(train.sequence, train.label, tokenizer)\n",
    "# test_dataset = SupervisedDataset(test.sequence, test.label, tokenizer)\n",
    "# val_dataset = SupervisedDataset(validation.sequence, validation.label, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy_score(labels, predictions),\n",
    "#         \"f1\": f1_score(\n",
    "#             labels, predictions, average=\"macro\", zero_division=0\n",
    "#         ),\n",
    "#         \"matthews_correlation\": matthews_corrcoef(\n",
    "#             labels, predictions\n",
    "#         ),\n",
    "#         \"precision\": precision_score(\n",
    "#             labels, predictions, average=\"macro\", zero_division=0\n",
    "#         ),\n",
    "#         \"recall\": recall_score(\n",
    "#             labels, predictions, average=\"macro\", zero_division=0\n",
    "#         ),\n",
    "#     }\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
    "#         logits = logits[0]\n",
    "#     return calculate_metric_with_sklearn(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # switch so it uses GPU, if not, will take ~50 hrs\n",
    "\n",
    "# train_args = TrainingArguments(seed = 42,\n",
    "#                                output_dir=\".\",\n",
    "#                                per_device_train_batch_size=16,\n",
    "#                                eval_strategy=\"epoch\",\n",
    "#                                learning_rate=0.000001,\n",
    "#                                num_train_epochs=4\n",
    "#                                )\n",
    "# trainer = transformers.Trainer(\n",
    "#                                 model=model,\n",
    "#                                 tokenizer=tokenizer,\n",
    "#                                 compute_metrics=compute_metrics,\n",
    "#                                 train_dataset=train_dataset,\n",
    "#                                 eval_dataset=val_dataset,\n",
    "#                                 args = train_args\n",
    "#                                 )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "  0%|          | 13/17100 [33:17<50:06:56, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6856247186660767, 'eval_accuracy': 0.5748713149274685, 'eval_f1': 0.460186084196384, 'eval_matthews_correlation': -0.035962858153736454, 'eval_precision': 0.4771693852995827, 'eval_recall': 0.48583779736598454, 'eval_runtime': 1748.6551, 'eval_samples_per_second': 4.888, 'eval_steps_per_second': 0.611, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# results = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6856247186660767,\n",
       " 'eval_accuracy': 0.5748713149274685,\n",
       " 'eval_f1': 0.460186084196384,\n",
       " 'eval_matthews_correlation': -0.035962858153736454,\n",
       " 'eval_precision': 0.4771693852995827,\n",
       " 'eval_recall': 0.48583779736598454,\n",
       " 'eval_runtime': 1748.6551,\n",
       " 'eval_samples_per_second': 4.888,\n",
       " 'eval_steps_per_second': 0.611,\n",
       " 'epoch': 0.0030409356725146198}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grover",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
