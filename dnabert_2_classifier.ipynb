{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032d64fb-708c-473a-ade7-9828256cb72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:57:48.896439: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-23 16:57:48.936705: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-23 16:57:48.936722: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-23 16:57:48.936745: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-23 16:57:48.951017: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kan019/.conda/envs/aam/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "from tensorflow import keras as K\n",
    "from aam.models.sequence_regressor import SequenceRegressor\n",
    "from aam.callbacks import SaveModel\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from biom import load_table, Table\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import json\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6fbe9-8150-4bb6-83c7-c3b2ef4902ff",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126954d3-e3a4-4971-8454-2bedce4b9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorEmbedding(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        table = None,\n",
    "        metadata = None,\n",
    "        metadata_column = None,\n",
    "        shift = None,\n",
    "        scale = \"minmax\",\n",
    "        max_token_per_sample: int = 1024,\n",
    "        shuffle: bool = False,\n",
    "        rarefy_depth: int = 5000,\n",
    "        epochs: int = 1000,\n",
    "        gen_new_tables: bool = False,\n",
    "        batch_size: int = 8,\n",
    "        max_bp: int = 150,\n",
    "        is_16S: bool = True,\n",
    "        is_categorical = None,\n",
    "        gen_new_table_frequency=3,\n",
    "        return_sample_ids=False,\n",
    "        tree_path=None,\n",
    "        seed=None,\n",
    "        asv_embeddings_fp=None\n",
    "    ):      \n",
    "        if isinstance(table, str):\n",
    "            table = load_table(table)\n",
    "        self.table: Table = table\n",
    "        self.tree_path = tree_path\n",
    "        self.is_categorical: bool = is_categorical\n",
    "        asv_embeddings = np.load(asv_embeddings_fp)\n",
    "        obs_ids = self.table.ids(axis=\"observation\")\n",
    "        self.asv_embeddings_dict = {k:v for k,v in zip(obs_ids, asv_embeddings)}\n",
    "        self.metadata_column: str = metadata_column\n",
    "        self.shift = shift\n",
    "        self.scale = scale\n",
    "        self.metadata: pd.Series = metadata\n",
    "        self.rarefy_depth: int = rarefy_depth\n",
    "        self.max_token_per_sample: int = max_token_per_sample\n",
    "        self.return_sample_ids: bool = return_sample_ids\n",
    "        self.include_sample_weight: bool = is_categorical\n",
    "        self.shuffle = shuffle\n",
    "        self.epochs = epochs\n",
    "        self.gen_new_tables = gen_new_tables\n",
    "        self.samples_per_minibatch = batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_bp = max_bp\n",
    "        self.is_16S = is_16S\n",
    "        self.seed = seed\n",
    "        self.gen_new_table_frequency = gen_new_table_frequency\n",
    "        self.epochs_since_last_table = 0\n",
    "        self.encoder_target = None\n",
    "        self.encoder_dtype = None\n",
    "        self.encoder_output_type = None\n",
    "        self.sample_ids = None\n",
    "        self.asv_ids = None\n",
    "        if self.tree_path is not None:\n",
    "            self.tree = to_skbio_treenode(parse_newick(open(self.tree_path).read()))\n",
    "            self.postorder_pos = {n.name: i for i, n in enumerate(self.tree.postorder()) if n.is_tip()}\n",
    "        print(\"rarefy table...\")\n",
    "        self.rarefied_table: Table = self.table.subsample(rarefy_depth)\n",
    "        self.size = self.rarefied_table.shape[1]\n",
    "        self.steps_per_epoch = self.size // self.batch_size\n",
    "        self.y_data = self.metadata.loc[self._rarefied_table.ids()]\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        sample_indices = self.sample_indices[start:end]\n",
    "        batch_sample_ids = self.sample_ids[sample_indices]\n",
    "        return self._batch_data(batch_sample_ids)\n",
    "    \n",
    "    def _batch_data(self, batch_sample_ids):\n",
    "        num_unique_asvs, sparse_indices, obs_indices, counts = [], [], [], []\n",
    "        cur_row_indx = 0\n",
    "        for s_id in batch_sample_ids:\n",
    "            sample_data = self.rarefied_table.data(s_id, dense=False).tocoo()\n",
    "            obs_idx, sample_counts = sample_data.row, sample_data.data\n",
    "            # remove zeros\n",
    "            non_zero_mask = sample_counts > 0.0\n",
    "            obs_idx = obs_idx[non_zero_mask]\n",
    "            sample_counts = sample_counts[non_zero_mask]\n",
    "            num_unique_asvs.append(len(obs_idx))\n",
    "            sparse_indices.append(([[cur_row_indx, i] for i in range(len(obs_idx))]))\n",
    "            obs_indices.append(obs_idx) \n",
    "            counts.append(sample_counts)\n",
    "            cur_row_indx += 1\n",
    "        num_unique_asvs = np.array(num_unique_asvs, dtype=np.int32)\n",
    "        sparse_indices = np.vstack(sparse_indices, dtype=np.int32)\n",
    "        obs_indices = np.hstack(obs_indices, dtype=np.int32)\n",
    "        counts = np.hstack(counts, dtype=np.float32)[:, np.newaxis]\n",
    "        \n",
    "        # get list of unique observations in batch\n",
    "        unique_obs, obs_indices = np.unique(obs_indices, return_inverse=True)\n",
    "        obs = self.rarefied_table.ids(axis=\"observation\")\n",
    "        asvs = obs[unique_obs]\n",
    "        y_true = self.y_data.loc[batch_sample_ids].to_numpy()[:, np.newaxis]\n",
    "        asvs_array = np.array([self.asv_embeddings_dict[asv] for asv in asvs])\n",
    "        batch_embeddings, counts = self.batch_embeddings(asvs_array, sparse_indices, counts, obs_indices)\n",
    "        return (batch_embeddings, counts), y_true\n",
    "    \n",
    "    def batch_embeddings(self, asv_embeddings, batch_indicies, counts, asv_indices=None):\n",
    "        emb_dim = tf.shape(asv_embeddings)[-1]\n",
    "        if asv_indices is not None:\n",
    "            asv_embeddings = tf.gather(asv_embeddings, asv_indices)\n",
    "        batch_shape = tf.reduce_max(batch_indicies[:, 0]) + 1\n",
    "        max_unique = tf.reduce_max(batch_indicies[:, 1]) + 1\n",
    "        batch_embeddings = tf.scatter_nd(\n",
    "            batch_indicies, asv_embeddings, shape=[batch_shape, max_unique, emb_dim]\n",
    "        )\n",
    "        counts = tf.scatter_nd(\n",
    "            batch_indicies, counts, shape=[batch_shape, max_unique, 1]\n",
    "        )\n",
    "        return batch_embeddings.numpy(), counts.numpy()\n",
    "        \n",
    "    def sort_using_counts(self, tensor, counts):\n",
    "        sorted_indices = tf.argsort(tf.squeeze(counts, axis=-1), axis=1, direction=\"DESCENDING\")\n",
    "        sorted_tensor = tf.gather(tensor, sorted_indices, axis=1, batch_dims=1)\n",
    "        sorted_counts = tf.gather(counts, sorted_indices, axis=1, batch_dims=1)\n",
    "        return sorted_tensor, sorted_counts\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.gen_new_tables and self.epochs_since_last_table > self.gen_new_table_frequency:\n",
    "            print(\"resampling dataset...\")\n",
    "            self.rarefied_table = self.table.subsample(self.rarefy_depth)\n",
    "            self.epochs_since_last_table = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.sample_indices)\n",
    "        self.epochs_since_last_table += 1\n",
    "    \n",
    "    @property\n",
    "    def rarefied_table(self):\n",
    "        return self._rarefied_table\n",
    "    \n",
    "    @rarefied_table.setter\n",
    "    def rarefied_table(self, table: Table):\n",
    "        self._rarefied_table = table\n",
    "        print(\"removing empty sample/obs from table\")\n",
    "        self._rarefied_table.remove_empty()\n",
    "        if self.tree_path is not None:\n",
    "            def sort_obs(obs):\n",
    "                post_pos = [self.postorder_pos[ob] for ob in obs]\n",
    "                sorted_indices = np.argsort(post_pos)\n",
    "                return obs[sorted_indices]\n",
    "            self._rarefied_table = self._rarefied_table.sort(sort_obs, axis=\"observation\")\n",
    "        self.sample_ids = self._rarefied_table.ids()\n",
    "        self.asv_ids = self._rarefied_table.ids(axis=\"observation\")\n",
    "        self.sample_indices = np.arange(len(self.sample_ids))\n",
    "        print(\"creating encoder target...\")\n",
    "        self.encoder_target = self._create_encoder_target()\n",
    "        print(\"encoder target created\")\n",
    "\n",
    "    def _create_encoder_target(self) -> None:\n",
    "        return None\n",
    "    \n",
    "    def _encoder_output(self, sample_ids):\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def table(self) -> Table:\n",
    "        return self._table\n",
    "    @table.setter\n",
    "    def table(self, table):\n",
    "        self._table = table\n",
    "    @property\n",
    "    def metadata(self) -> pd.Series:\n",
    "        return self._metadata\n",
    "    @metadata.setter\n",
    "    def metadata(self, metadata):\n",
    "        if metadata is None:\n",
    "            return\n",
    "        if isinstance(metadata, str):\n",
    "            metadata = pd.read_csv(metadata, sep=\"\\t\", index_col=0, dtype={0: str})\n",
    "        if self.metadata_column not in metadata.columns:\n",
    "            raise Exception(f\"Invalid metadata column {self.metadata_column}\")\n",
    "        print(\"aligning table with metadata\")\n",
    "        samp_ids = np.intersect1d(self.table.ids(axis=\"sample\"), metadata.index)\n",
    "        self.table.filter(samp_ids, axis=\"sample\", inplace=True)\n",
    "        self.table.remove_empty()\n",
    "        metadata = metadata.loc[self.table.ids(), self.metadata_column]\n",
    "        print(f\"aligned table shape: {self.table.shape}\")\n",
    "        print(f\"aligned metadata shape: {metadata.shape}\")\n",
    "        metadata = metadata.astype(np.int32)\n",
    "        self._metadata = metadata.reindex(self.table.ids())\n",
    "        print(\"done preprocessing metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5e6e8-532d-43ae-a4b7-9c3049f9d078",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fc4135-ce2f-409f-ac1d-738e51e3e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(K.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.model = SequenceRegressor(\n",
    "            2048, dropout_rate=0.1, embedding_dim = 768, intermediate_size = 3072, \n",
    "            intermediate_activation = \"gelu\", use_residual_connections = False, out_dim = 1\n",
    "        )\n",
    "\n",
    "        self.loss_fn = K.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        self.auc_tracker = K.metrics.AUC(from_logits=True)\n",
    "        self.loss_tracker = K.metrics.Mean()\n",
    "\n",
    "    def call(self, inputs, mask=None, training=False):\n",
    "        \"\"\"\n",
    "        inputs: [B, A, N], B: batch_dim, A: # ASV in sample, N: nuctides,\n",
    "        string tensor\n",
    "        \"\"\"\n",
    "        return self.model(inputs, training=training)\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        # create attention mask\n",
    "        # example [[\"ACTG\"], [\"\"]]\n",
    "        with tf.GradientTape() as tape:\n",
    "            sample_embeddings, output = self(x, training=True)\n",
    "            loss = self.loss_fn(y, output)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.auc_tracker.update_state(y, output)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"auc\": self.auc_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        sample_embeddings, output = self(x, training=False)\n",
    "        loss = self.loss_fn(y, output)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.auc_tracker.update_state(y, output)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"auc\": self.auc_tracker.result()}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        x, y = data\n",
    "        sample_embeddings, output = self(x, training=False)\n",
    "        predictions = tf.reshape(tf.keras.activations.sigmoid(output), shape=[-1])\n",
    "        y = tf.reshape(y, shape=[-1])\n",
    "        y = tf.cast(y, dtype=tf.float32)\n",
    "        pred_label = tf.cast(predictions >= 0.5, dtype=tf.float32)\n",
    "        correct = tf.reshape(tf.cast(pred_label == y, dtype=tf.float32), shape=[-1])\n",
    "        correct = tf.reduce_mean(correct, axis=0)\n",
    "        return pred_label, y, correct\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Classifier, self).get_config()\n",
    "        config.update({'build_input_shape': self.get_build_config()})\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.built:\n",
    "            return\n",
    "        super(Classifier, self).build(input_shape)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        build_input_shape = config.pop('build_input_shape')\n",
    "        input_shape = build_input_shape['input_shape']\n",
    "        config['feature_extractor'] = tf.keras.saving.deserialize_keras_object(config['feature_extractor'])\n",
    "        model = cls(**config)\n",
    "        model.build(input_shape)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc561f-50f7-4688-8d99-acd58960b7b8",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e70e8b-3313-4bed-a7cc-151cb6f01ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_type(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    # Remove the 'training_metadata_' prefix and the file extension\n",
    "    if filename.startswith('training_metadata_'):\n",
    "        sample_type = filename[len('training_metadata_'):]\n",
    "        sample_type = os.path.splitext(sample_type)[0]\n",
    "        return sample_type\n",
    "    return \"Unknown\"\n",
    "    \n",
    "#function that creates training and valid split and trains each model\n",
    "def train_model(train_fp, model=None):\n",
    "    print()\n",
    "    training_metadata = pd.read_csv(train_fp, sep='\\t', index_col=0)\n",
    "    X = training_metadata.drop(columns=['study_sample_type', 'has_covid'], axis=1)\n",
    "    y = training_metadata[['study_sample_type', 'has_covid']]\n",
    "    \n",
    "    _, _, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    curr_best_val_loss = np.inf\n",
    "    curr_best_model = None\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(y, y['has_covid'])):\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_valid = y.iloc[valid_index]\n",
    "    \n",
    "        embed_train = GeneratorEmbedding(\n",
    "            table='data/input/merged_biom_table.biom',\n",
    "            metadata=y_train,\n",
    "            metadata_column='has_covid',\n",
    "            shuffle=False,\n",
    "            is_categorical=False,\n",
    "            shift=0,\n",
    "            rarefy_depth = 4000,\n",
    "            scale=1,\n",
    "            batch_size = 4,\n",
    "            gen_new_tables = True, #only in training dataset\n",
    "            asv_embeddings_fp = \"asv_embeddings.npy\"\n",
    "            \n",
    "        )\n",
    "    \n",
    "        embed_valid = GeneratorEmbedding(\n",
    "            table='data/input/merged_biom_table.biom',\n",
    "            metadata=y_valid,\n",
    "            metadata_column='has_covid',\n",
    "            shuffle=False,\n",
    "            is_categorical=False,\n",
    "            shift=0,\n",
    "            rarefy_depth = 4000,\n",
    "            scale=1,\n",
    "            batch_size = 4,\n",
    "            asv_embeddings_fp = \"asv_embeddings.npy\"\n",
    "        )\n",
    "\n",
    "        def get_sample_type(file_path):\n",
    "            filename = os.path.basename(file_path)\n",
    "            # Remove the 'training_metadata_' prefix and the file extension\n",
    "            if filename.startswith('training_metadata_'):\n",
    "                sample_type = filename[len('training_metadata_'):]\n",
    "                sample_type = os.path.splitext(sample_type)[0]\n",
    "                return sample_type\n",
    "            return \"Unknown\"\n",
    "\n",
    "        model = Classifier()\n",
    "        asv_embedding_shape = tf.TensorShape([None, None, 768])\n",
    "        count_shape = tf.TensorShape([None, None, 1])\n",
    "        model.build([asv_embedding_shape, count_shape])\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=tf.keras.optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate = 0.0,\n",
    "            warmup_target = 0.0003,\n",
    "            warmup_steps=0,\n",
    "            decay_steps=100_000\n",
    "        )\n",
    ")\n",
    "        model.compile(optimizer=optimizer, run_eagerly=False)\n",
    "        #switch loss to val loss \n",
    "        #pass early stopping for callbacks\n",
    "        history = model.fit(embed_train, \n",
    "                  validation_data = embed_valid, \n",
    "                  validation_steps=embed_valid.steps_per_epoch, \n",
    "                  epochs=10, \n",
    "                  steps_per_epoch=embed_train.steps_per_epoch, \n",
    "                  callbacks=[\n",
    "                          SaveModel(\"model_test.keras\", report_back=1),\n",
    "                          EarlyStopping(patience=250, start_from_epoch=0, restore_best_weights=True)\n",
    "                   ])\n",
    "\n",
    "        validation_loss = history.history['val_loss']\n",
    "        epochs = np.array(range(len(validation_loss)))\n",
    "        plt.plot(epochs, validation_loss)\n",
    "        plt.title(f'Validation Loss Per Epoch {validation_loss[-1]}')\n",
    "        plt.savefig(f'trained_models/{get_sample_type(train_fp)}/{get_sample_type(train_fp)}_{i}_model_loss.png')\n",
    "        plt.close()\n",
    "        if history.history['val_loss'][-1] < curr_best_val_loss:\n",
    "            curr_best_model = model\n",
    "        model.save(f'trained_models/{get_sample_type(train_fp)}/{get_sample_type(train_fp)}_{i}_model.keras', save_format='keras')\n",
    "    curr_best_model.save(f'trained_models/{get_sample_type(train_fp)}/{get_sample_type(train_fp)}_best_model.keras', save_format='keras')\n",
    "    print(f\"\\nDNABERT-2: Best model saved for {get_sample_type(train_fp)} samples.\")\n",
    "\n",
    "def run_dnabert_2():\n",
    "    train_model('data/input/training_metadata_forehead.tsv')\n",
    "    train_model('data/input/training_metadata_inside_floor.tsv')\n",
    "    train_model('data/input/training_metadata_stool.tsv')\n",
    "    train_model('data/input/training_metadata_nares.tsv')\n",
    "    print(f\"\\nDNABERT-2: Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99400403-1d6e-4a22-9056-383109d02310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "aligning table with metadata\n",
      "aligned table shape: (1882, 53)\n",
      "aligned metadata shape: (53,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (804, 14)\n",
      "aligned metadata shape: (14,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:57:57.586927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9804 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:58:00.042041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2025-02-23 16:58:02.284576: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f3d5dfaa520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-23 16:58:02.284591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-02-23 16:58:02.289490: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-23 16:58:02.463107: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 73ms/step - loss: 0.6782 - auc: 0.4883 - val_loss: 0.6961 - val_auc: 0.1667 - iteration: 10.0000 - best_metric: 0.6961\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.4839 - auc: 0.6992 - val_loss: 0.6740 - val_auc: 0.1667 - iteration: 20.0000 - best_metric: 0.6740\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.5029 - auc: 0.6641 - val_loss: 0.6586 - val_auc: 0.1667 - iteration: 30.0000 - best_metric: 0.6586\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.3523 - auc: 0.8691 - val_loss: 0.6446 - val_auc: 0.2500 - iteration: 40.0000 - best_metric: 0.6446\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.2597 - auc: 0.9180 - val_loss: 0.6322 - val_auc: 0.2917 - iteration: 50.0000 - best_metric: 0.6322\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.3608 - auc: 0.9043 - val_loss: 0.6166 - val_auc: 0.3750 - iteration: 60.0000 - best_metric: 0.6166\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.2851 - auc: 0.9141 - val_loss: 0.6019 - val_auc: 0.3750 - iteration: 70.0000 - best_metric: 0.6019\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 49ms/step - loss: 0.2601 - auc: 0.9746 - val_loss: 0.5909 - val_auc: 0.4167 - iteration: 80.0000 - best_metric: 0.5909\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.2396 - auc: 0.9688 - val_loss: 0.5786 - val_auc: 0.4167 - iteration: 90.0000 - best_metric: 0.5786\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 49ms/step - loss: 0.2872 - auc: 0.9922 - val_loss: 0.5736 - val_auc: 0.4167 - iteration: 100.0000 - best_metric: 0.5736\n",
      "aligning table with metadata\n",
      "aligned table shape: (1951, 53)\n",
      "aligned metadata shape: (53,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (728, 14)\n",
      "aligned metadata shape: (14,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 2s 66ms/step - loss: 0.9790 - auc: 0.3774 - val_loss: 0.3362 - val_auc: 0.0000e+00 - iteration: 10.0000 - best_metric: 0.3362\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.5175 - auc: 0.7492 - val_loss: 0.2551 - val_auc: 0.0000e+00 - iteration: 20.0000 - best_metric: 0.2551\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.5819 - auc: 0.5490 - val_loss: 0.1998 - val_auc: 0.0000e+00 - iteration: 30.0000 - best_metric: 0.1998\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.4789 - auc: 0.8157 - val_loss: 0.1672 - val_auc: 0.0000e+00 - iteration: 40.0000 - best_metric: 0.1672\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.4260 - auc: 0.7870 - val_loss: 0.1403 - val_auc: 0.0000e+00 - iteration: 50.0000 - best_metric: 0.1403\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.3174 - auc: 0.8587 - val_loss: 0.1189 - val_auc: 0.0000e+00 - iteration: 60.0000 - best_metric: 0.1189\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.4404 - auc: 0.7235 - val_loss: 0.1064 - val_auc: 0.0000e+00 - iteration: 70.0000 - best_metric: 0.1064\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.2819 - auc: 0.9535 - val_loss: 0.0944 - val_auc: 0.0000e+00 - iteration: 80.0000 - best_metric: 0.0944\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.2177 - auc: 0.8601 - val_loss: 0.0848 - val_auc: 0.0000e+00 - iteration: 90.0000 - best_metric: 0.0848\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.3388 - auc: 0.9527 - val_loss: 0.0781 - val_auc: 0.0000e+00 - iteration: 100.0000 - best_metric: 0.0781\n",
      "aligning table with metadata\n",
      "aligned table shape: (1918, 54)\n",
      "aligned metadata shape: (54,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (728, 13)\n",
      "aligned metadata shape: (13,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 2s 75ms/step - loss: 0.7259 - auc: 0.6978 - val_loss: 0.7094 - val_auc: 0.4500 - iteration: 9.0000 - best_metric: 0.7094\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.6621 - auc: 0.5214 - val_loss: 0.5982 - val_auc: 0.5000 - iteration: 18.0000 - best_metric: 0.5982\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 55ms/step - loss: 0.4353 - auc: 0.7152 - val_loss: 0.5312 - val_auc: 0.5000 - iteration: 27.0000 - best_metric: 0.5312\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.4882 - auc: 0.7560 - val_loss: 0.4950 - val_auc: 0.7000 - iteration: 36.0000 - best_metric: 0.4950\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 54ms/step - loss: 0.3560 - auc: 0.8244 - val_loss: 0.4656 - val_auc: 0.7250 - iteration: 45.0000 - best_metric: 0.4656\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.2213 - auc: 0.9769 - val_loss: 0.4514 - val_auc: 0.7750 - iteration: 54.0000 - best_metric: 0.4514\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.3008 - auc: 0.9472 - val_loss: 0.4448 - val_auc: 0.7500 - iteration: 63.0000 - best_metric: 0.4448\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.1611 - auc: 0.8951 - val_loss: 0.4434 - val_auc: 0.7250 - iteration: 72.0000 - best_metric: 0.4434\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 0.2780 - auc: 0.8597 - val_loss: 0.4446 - val_auc: 0.7250 - iteration: 81.0000 - best_metric: 0.4434\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.2273 - auc: 0.8635 - val_loss: 0.4467 - val_auc: 0.7250 - iteration: 90.0000 - best_metric: 0.4434\n",
      "aligning table with metadata\n",
      "aligned table shape: (1873, 54)\n",
      "aligned metadata shape: (54,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (777, 13)\n",
      "aligned metadata shape: (13,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 2s 66ms/step - loss: 0.9868 - auc: 0.5087 - val_loss: 0.8521 - val_auc: 0.8750 - iteration: 10.0000 - best_metric: 0.8521\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.7753 - auc: 0.5450 - val_loss: 0.6981 - val_auc: 0.8333 - iteration: 20.0000 - best_metric: 0.6981\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.5305 - auc: 0.7798 - val_loss: 0.6220 - val_auc: 0.8750 - iteration: 30.0000 - best_metric: 0.6220\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.3924 - auc: 0.9439 - val_loss: 0.5754 - val_auc: 0.8333 - iteration: 40.0000 - best_metric: 0.5754\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.3917 - auc: 0.8267 - val_loss: 0.5532 - val_auc: 0.8333 - iteration: 50.0000 - best_metric: 0.5532\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 0.2823 - auc: 0.9548 - val_loss: 0.5503 - val_auc: 0.8333 - iteration: 60.0000 - best_metric: 0.5503\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.2620 - auc: 0.8710 - val_loss: 0.5537 - val_auc: 0.8333 - iteration: 70.0000 - best_metric: 0.5503\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.2357 - auc: 0.8651 - val_loss: 0.5589 - val_auc: 0.8333 - iteration: 80.0000 - best_metric: 0.5503\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.2090 - auc: 0.8131 - val_loss: 0.5687 - val_auc: 0.8333 - iteration: 90.0000 - best_metric: 0.5503\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.2720 - auc: 0.9934 - val_loss: 0.5706 - val_auc: 0.8333 - iteration: 100.0000 - best_metric: 0.5503\n",
      "aligning table with metadata\n",
      "aligned table shape: (1987, 54)\n",
      "aligned metadata shape: (54,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (612, 13)\n",
      "aligned metadata shape: (13,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 2s 69ms/step - loss: 0.9737 - auc: 0.4045 - val_loss: 0.5801 - val_auc: 0.5833 - iteration: 10.0000 - best_metric: 0.5801\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.6417 - auc: 0.7094 - val_loss: 0.5646 - val_auc: 0.5833 - iteration: 20.0000 - best_metric: 0.5646\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 48ms/step - loss: 0.5305 - auc: 0.5875 - val_loss: 0.5682 - val_auc: 0.4583 - iteration: 30.0000 - best_metric: 0.5646\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 49ms/step - loss: 0.4163 - auc: 0.7614 - val_loss: 0.5830 - val_auc: 0.3750 - iteration: 40.0000 - best_metric: 0.5646\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 49ms/step - loss: 0.4448 - auc: 0.8081 - val_loss: 0.6028 - val_auc: 0.3750 - iteration: 50.0000 - best_metric: 0.5646\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.3126 - auc: 0.9485 - val_loss: 0.6200 - val_auc: 0.3333 - iteration: 60.0000 - best_metric: 0.5646\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 49ms/step - loss: 0.2749 - auc: 0.9264 - val_loss: 0.6375 - val_auc: 0.2917 - iteration: 70.0000 - best_metric: 0.5646\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.2675 - auc: 0.7093 - val_loss: 0.6465 - val_auc: 0.2500 - iteration: 80.0000 - best_metric: 0.5646\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.2836 - auc: 0.6865 - val_loss: 0.6625 - val_auc: 0.2500 - iteration: 90.0000 - best_metric: 0.5646\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.2587 - auc: 0.8047 - val_loss: 0.6786 - val_auc: 0.2917 - iteration: 100.0000 - best_metric: 0.5646\n",
      "\n",
      "DNABERT-2: Best model saved for forehead samples.\n",
      "\n",
      "aligning table with metadata\n",
      "aligned table shape: (21678, 76)\n",
      "aligned metadata shape: (76,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (5487, 20)\n",
      "aligned metadata shape: (20,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 3s 78ms/step - loss: 0.6385 - auc: 0.6855 - val_loss: 0.6665 - val_auc: 0.4250 - iteration: 15.0000 - best_metric: 0.6665\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.5418 - auc: 0.7792 - val_loss: 0.6689 - val_auc: 0.4083 - iteration: 30.0000 - best_metric: 0.6665\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4638 - auc: 0.8625 - val_loss: 0.6832 - val_auc: 0.4250 - iteration: 45.0000 - best_metric: 0.6665\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.5003 - auc: 0.8111 - val_loss: 0.6914 - val_auc: 0.4250 - iteration: 60.0000 - best_metric: 0.6665\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.5196 - auc: 0.7956 - val_loss: 0.7009 - val_auc: 0.4167 - iteration: 75.0000 - best_metric: 0.6665\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4504 - auc: 0.8893 - val_loss: 0.6971 - val_auc: 0.4167 - iteration: 90.0000 - best_metric: 0.6665\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.3996 - auc: 0.9241 - val_loss: 0.7270 - val_auc: 0.4000 - iteration: 105.0000 - best_metric: 0.6665\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.4153 - auc: 0.8892 - val_loss: 0.7483 - val_auc: 0.4083 - iteration: 120.0000 - best_metric: 0.6665\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.3173 - auc: 0.9629 - val_loss: 0.7300 - val_auc: 0.4000 - iteration: 135.0000 - best_metric: 0.6665\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.2801 - auc: 0.9892 - val_loss: 0.7112 - val_auc: 0.3750 - iteration: 150.0000 - best_metric: 0.6665\n",
      "aligning table with metadata\n",
      "aligned table shape: (19829, 77)\n",
      "aligned metadata shape: (77,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (8315, 19)\n",
      "aligned metadata shape: (19,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 3s 78ms/step - loss: 0.8320 - auc: 0.3967 - val_loss: 0.8717 - val_auc: 0.6909 - iteration: 15.0000 - best_metric: 0.8717\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 51ms/step - loss: 0.6082 - auc: 0.7208 - val_loss: 0.9222 - val_auc: 0.7273 - iteration: 30.0000 - best_metric: 0.8717\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.4813 - auc: 0.8589 - val_loss: 0.8889 - val_auc: 0.7000 - iteration: 45.0000 - best_metric: 0.8717\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.5386 - auc: 0.8034 - val_loss: 0.9239 - val_auc: 0.6636 - iteration: 60.0000 - best_metric: 0.8717\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4798 - auc: 0.8872 - val_loss: 0.9723 - val_auc: 0.6545 - iteration: 75.0000 - best_metric: 0.8717\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.4084 - auc: 0.9348 - val_loss: 0.9485 - val_auc: 0.6182 - iteration: 90.0000 - best_metric: 0.8717\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4193 - auc: 0.9200 - val_loss: 0.9132 - val_auc: 0.6091 - iteration: 105.0000 - best_metric: 0.8717\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.4372 - auc: 0.8875 - val_loss: 0.9714 - val_auc: 0.6455 - iteration: 120.0000 - best_metric: 0.8717\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.4150 - auc: 0.8981 - val_loss: 1.0558 - val_auc: 0.5273 - iteration: 135.0000 - best_metric: 0.8717\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.4416 - auc: 0.9057 - val_loss: 0.9965 - val_auc: 0.4455 - iteration: 150.0000 - best_metric: 0.8717\n",
      "aligning table with metadata\n",
      "aligned table shape: (20355, 77)\n",
      "aligned metadata shape: (77,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (7603, 19)\n",
      "aligned metadata shape: (19,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 3s 79ms/step - loss: 0.9277 - auc: 0.4437 - val_loss: 0.8528 - val_auc: 0.4444 - iteration: 15.0000 - best_metric: 0.8528\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6001 - auc: 0.7694 - val_loss: 0.8441 - val_auc: 0.3651 - iteration: 30.0000 - best_metric: 0.8441\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.5438 - auc: 0.7126 - val_loss: 0.8052 - val_auc: 0.3254 - iteration: 45.0000 - best_metric: 0.8052\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4976 - auc: 0.7690 - val_loss: 0.7956 - val_auc: 0.2302 - iteration: 60.0000 - best_metric: 0.7956\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.5433 - auc: 0.7765 - val_loss: 0.7528 - val_auc: 0.3651 - iteration: 75.0000 - best_metric: 0.7528\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.5977 - auc: 0.7370 - val_loss: 0.7650 - val_auc: 0.2460 - iteration: 90.0000 - best_metric: 0.7528\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.5887 - auc: 0.8298 - val_loss: 0.7323 - val_auc: 0.2540 - iteration: 105.0000 - best_metric: 0.7323\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 0.4329 - auc: 0.7942 - val_loss: 0.7493 - val_auc: 0.2937 - iteration: 120.0000 - best_metric: 0.7323\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.3938 - auc: 0.9330 - val_loss: 0.7519 - val_auc: 0.3492 - iteration: 135.0000 - best_metric: 0.7323\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.4466 - auc: 0.7681 - val_loss: 0.7768 - val_auc: 0.3492 - iteration: 150.0000 - best_metric: 0.7323\n",
      "aligning table with metadata\n",
      "aligned table shape: (20383, 77)\n",
      "aligned metadata shape: (77,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (7288, 19)\n",
      "aligned metadata shape: (19,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 3s 72ms/step - loss: 0.8652 - auc: 0.4103 - val_loss: 0.7379 - val_auc: 0.5556 - iteration: 16.0000 - best_metric: 0.7379\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.6478 - auc: 0.7300 - val_loss: 0.7175 - val_auc: 0.5833 - iteration: 32.0000 - best_metric: 0.7175\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.6109 - auc: 0.6884 - val_loss: 0.7286 - val_auc: 0.6389 - iteration: 48.0000 - best_metric: 0.7175\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5056 - auc: 0.8581 - val_loss: 0.7604 - val_auc: 0.6944 - iteration: 64.0000 - best_metric: 0.7175\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.4467 - auc: 0.8624 - val_loss: 0.7607 - val_auc: 0.6528 - iteration: 80.0000 - best_metric: 0.7175\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.4485 - auc: 0.8549 - val_loss: 0.7316 - val_auc: 0.6944 - iteration: 96.0000 - best_metric: 0.7175\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.3424 - auc: 0.9324 - val_loss: 0.7263 - val_auc: 0.7083 - iteration: 112.0000 - best_metric: 0.7175\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.4887 - auc: 0.8694 - val_loss: 0.7450 - val_auc: 0.6806 - iteration: 128.0000 - best_metric: 0.7175\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.4053 - auc: 0.9052 - val_loss: 0.7189 - val_auc: 0.7222 - iteration: 144.0000 - best_metric: 0.7175\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.4274 - auc: 0.8726 - val_loss: 0.6912 - val_auc: 0.7361 - iteration: 160.0000 - best_metric: 0.6912\n",
      "aligning table with metadata\n",
      "aligned table shape: (19617, 77)\n",
      "aligned metadata shape: (77,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (8351, 19)\n",
      "aligned metadata shape: (19,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 3s 69ms/step - loss: 0.8490 - auc: 0.4699 - val_loss: 1.0006 - val_auc: 0.7778 - iteration: 16.0000 - best_metric: 1.0006\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.5494 - auc: 0.7678 - val_loss: 0.9470 - val_auc: 0.7083 - iteration: 32.0000 - best_metric: 0.9470\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.5749 - auc: 0.7151 - val_loss: 0.9345 - val_auc: 0.7639 - iteration: 48.0000 - best_metric: 0.9345\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5121 - auc: 0.7778 - val_loss: 0.9149 - val_auc: 0.7083 - iteration: 64.0000 - best_metric: 0.9149\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.7125 - auc: 0.6403 - val_loss: 0.9586 - val_auc: 0.7639 - iteration: 80.0000 - best_metric: 0.9149\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.7191 - auc: 0.6377 - val_loss: 0.9710 - val_auc: 0.7083 - iteration: 96.0000 - best_metric: 0.9149\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.5551 - auc: 0.8170 - val_loss: 0.9045 - val_auc: 0.7500 - iteration: 112.0000 - best_metric: 0.9045\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.5115 - auc: 0.7871 - val_loss: 0.8557 - val_auc: 0.7639 - iteration: 128.0000 - best_metric: 0.8557\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.5773 - auc: 0.7674 - val_loss: 0.8337 - val_auc: 0.8056 - iteration: 144.0000 - best_metric: 0.8337\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.5463 - auc: 0.8352 - val_loss: 0.8417 - val_auc: 0.7639 - iteration: 160.0000 - best_metric: 0.8337\n",
      "\n",
      "DNABERT-2: Best model saved for inside_floor samples.\n",
      "\n",
      "aligning table with metadata\n",
      "aligned table shape: (1906, 28)\n",
      "aligned metadata shape: (28,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (649, 7)\n",
      "aligned metadata shape: (7,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 2s 85ms/step - loss: 0.7293 - auc: 0.5907 - val_loss: 0.2405 - val_auc: 0.0000e+00 - iteration: 7.0000 - best_metric: 0.2405\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 0.7527 - auc: 0.4863 - val_loss: 0.2082 - val_auc: 0.0000e+00 - iteration: 14.0000 - best_metric: 0.2082\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.6931 - auc: 0.6538 - val_loss: 0.1883 - val_auc: 0.0000e+00 - iteration: 21.0000 - best_metric: 0.1883\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.5029 - auc: 0.7161 - val_loss: 0.1760 - val_auc: 0.0000e+00 - iteration: 28.0000 - best_metric: 0.1760\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.3847 - auc: 0.7889 - val_loss: 0.1599 - val_auc: 0.0000e+00 - iteration: 35.0000 - best_metric: 0.1599\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.3697 - auc: 0.9028 - val_loss: 0.1503 - val_auc: 0.0000e+00 - iteration: 42.0000 - best_metric: 0.1503\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.3601 - auc: 0.7911 - val_loss: 0.1371 - val_auc: 0.0000e+00 - iteration: 49.0000 - best_metric: 0.1371\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.3011 - auc: 0.9304 - val_loss: 0.1259 - val_auc: 0.0000e+00 - iteration: 56.0000 - best_metric: 0.1259\n",
      "resampling dataset...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 0.2939 - auc: 0.9802 - val_loss: 0.1216 - val_auc: 0.0000e+00 - iteration: 63.0000 - best_metric: 0.1216\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.3878 - auc: 0.8762 - val_loss: 0.1142 - val_auc: 0.0000e+00 - iteration: 70.0000 - best_metric: 0.1142\n",
      "aligning table with metadata\n",
      "aligned table shape: (1755, 28)\n",
      "aligned metadata shape: (28,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "aligning table with metadata\n",
      "aligned table shape: (955, 7)\n",
      "aligned metadata shape: (7,)\n",
      "done preprocessing metadata\n",
      "rarefy table...\n",
      "removing empty sample/obs from table\n",
      "creating encoder target...\n",
      "encoder target created\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "run_dnabert_2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aam",
   "language": "python",
   "name": "aam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
